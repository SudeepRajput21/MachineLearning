{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captioning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "448WUxW-nEqm"
      },
      "source": [
        "!pip3 install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3MEahPduVVY"
      },
      "source": [
        "!mkdir .kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THIPDYzJuYHZ"
      },
      "source": [
        "!mkdir ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aoic4SJzubtA"
      },
      "source": [
        "import json\n",
        "token = {\"username\":\"sudeeprajput21\",\"key\":\"1766481fd53783ab03477079054a9b6c\"}\n",
        "with open('/content/.kaggle/kaggle.json','w') as file:\n",
        "  json.dump(token, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkch2nkYuffH"
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lebO6248ujCx"
      },
      "source": [
        "!kaggle config set -n path -v{/content}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPONAd7mCT3f"
      },
      "source": [
        "Change permission.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGTXN07zunmg"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV2LIIDZuq_J"
      },
      "source": [
        "!kaggle datasets download -d srbhshinde/flickr8k-sau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWCWVM3TuwRg"
      },
      "source": [
        "!kaggle datasets download -d yliu9999/glove6b50d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUS5HheDu-RA"
      },
      "source": [
        "!cd /content/{/content}/datasets "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZJTrKFFvIVp"
      },
      "source": [
        "!unzip /content/{/content}/datasets/yliu9999/glove6b50d/glove6b50d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaOgDUXmwZ_K"
      },
      "source": [
        "!unzip /content/{/content}/datasets/srbhshinde/flickr8k-sau/flickr8k-sau.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KWne2RLwm6P"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxpAc-vxw0qH"
      },
      "source": [
        "cd Flickr_Data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY-u7c_-xYN8"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3HFopf2xcQS"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8tZJrnvxgsk"
      },
      "source": [
        "flickr_path='Flickr_Data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN78FfG9xzXL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords                 # will collect stopwords for english language\n",
        "import string\n",
        "import json\n",
        "from time import time\n",
        "import pickle                                     # helps storing different datatypes like list, arrays & helps serializing them\n",
        "from keras.applications.vgg16 import VGG16        # pretrained model for CNN\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions          #pretrained model for RNN\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical             # for categorical division\n",
        "from keras.layers import Input, Dense, Dropout, Embedding,LSTM\n",
        "from keras.layers.merge import add\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF9xjvHzzoRO"
      },
      "source": [
        "with open(flickr_path + \"Flickr_TextData/Flickr8k.token.txt\") as filepath:\n",
        "  captions = filepath.read()\n",
        "  filepath.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gylJumau1kdn"
      },
      "source": [
        "captions = captions.split (\"\\n\") [:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyPwXRPb1w0o"
      },
      "source": [
        "len(captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foNbpzdV1z52"
      },
      "source": [
        "captions[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbaorVQBDiDh"
      },
      "source": [
        "- Creating a dictionary where key is 'img_name' & value is list of captions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaCKl17g13Wp"
      },
      "source": [
        " descriptions = {}                          # description contains the name of the image in jpg\n",
        "\n",
        "for element in captions:\n",
        "  img_to_cap = element.split(\"\\t\")\n",
        "  img_name = img_to_cap[0].split(\".\")[0]\n",
        "  cap = img_to_cap[1]\n",
        "\n",
        "  if descriptions.get(img_name) == None:\n",
        "    descriptions[img_name] = []\n",
        "\n",
        "  descriptions[img_name].append(cap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t20vXjMC3wAY"
      },
      "source": [
        "descriptions['1000268201_693b08cb0e']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4B0g1nk5FGT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgNKQXo9498F"
      },
      "source": [
        "**Data Cleaning**\n",
        "- Lower each word.\n",
        "- Remove punctuations.\n",
        "- Remove words less than length 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcZTxEZ_5DQi"
      },
      "source": [
        "def clean_text(sample):                                   \n",
        "  sample = sample.lower()\n",
        "\n",
        "  sample = re.sub(\"[^a-z]+\",\" \", sample)\n",
        "\n",
        "  sample = sample.split()\n",
        "\n",
        "  sample = [s for s in sample if len(s)>1]\n",
        "\n",
        "  sample = \" \".join(sample)\n",
        "\n",
        "  return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv9Mjz1T6nuO"
      },
      "source": [
        "clean_text (\"I am Sudeep 21 03 rAjput @1996\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi-G2M038IWO"
      },
      "source": [
        "# Modify all the captions (cleaned captions)\n",
        "\n",
        "for key, desc_list in descriptions.items():\n",
        "  for i in range (len(desc_list)):\n",
        "    desc_list[i] = clean_text(desc_list[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3ydASql8pOj"
      },
      "source": [
        "descriptions ['1000268201_693b08cb0e']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frvGfoqA_0tp"
      },
      "source": [
        "# Writing clean descriptions to .txt file\n",
        "\n",
        "f = open(\"descriptions.txt\",\"w\")\n",
        "f.write(str(descriptions))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O2tZB8bAaHb"
      },
      "source": [
        "# Reading descriptions file to check captions\n",
        "\n",
        "f = open(\"descriptions.txt\",\"r\")\n",
        "descriptions = f.read()\n",
        "f.close()\n",
        "\n",
        "json_acceptable_string = descriptions.replace(\"'\",\"\\\"\")\n",
        "descriptions = json.loads(json_acceptable_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uk8gsi7CHjQ"
      },
      "source": [
        "descriptions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPeRxAxGBWpx"
      },
      "source": [
        "# Finding unique vocabulary (finding how many unique words are present in 40K captions)\n",
        "\n",
        "vocabulary = set()\n",
        "\n",
        "for key in descriptions.keys():\n",
        "  [vocabulary.update(i.split()) for i in descriptions[key]]\n",
        "\n",
        "print('Vocabulary Size:%d' %len(vocabulary))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4gNI8QqImSr"
      },
      "source": [
        "# Write all words in description dictionary\n",
        "\n",
        "all_vocab = []\n",
        "\n",
        "for key in descriptions.keys():\n",
        "  [all_vocab.append(i) for des in descriptions[key] for i in des.split()]\n",
        "\n",
        "print ('Vocabulary Size:%d' %len(all_vocab))\n",
        "print (all_vocab[:15])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMRSLp-vE3pk"
      },
      "source": [
        "- Count the frequency of each word, sort them & discard the words having frequency lesser than the threshold value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaqzXP22T2hw"
      },
      "source": [
        "import collections                                      # Create a vocabulary which will have atleast 10 times the word is occuring in entire process\n",
        "                                                        # Create a threshold for it.\n",
        "counter = collections.Counter(all_vocab)\n",
        "\n",
        "dict_ = dict(counter)\n",
        "\n",
        "threshold_value = 10\n",
        "\n",
        "sorted_dict = sorted(dict_.items(), reverse = True, key = lambda x: x[1])\n",
        "sorted_dict = [x for x in sorted_dict if x[1] > threshold_value]\n",
        "all_vocab = [x[0] for x in sorted_dict]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jz40fN_VPQB",
        "outputId": "faf7924c-6ff2-412a-c012-7edfb225cce4"
      },
      "source": [
        "len(all_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnBFKtDVFZ5G"
      },
      "source": [
        "- Now we have trained .txt file so we will train image file. Open the file & read it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3IW9iRdV6Ht"
      },
      "source": [
        "# Train image file\n",
        "\n",
        "f = open (flickr_path + \"Flickr_TextData/Flickr_8k.trainImages.txt\")\n",
        "train = f.read()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5msA2mOWYtj"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DccOgWUIWeXT"
      },
      "source": [
        "train = [e.split(\".\")[0] for e in train.split(\"\\n\")[:-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp8OBTGZWzRf"
      },
      "source": [
        "# Test image file (Reading test image file)\n",
        "\n",
        "f = open (flickr_path + \"Flickr_TextData/Flickr_8k.testImages.txt\")\n",
        "test = f.read()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiMcDc3KXDz0"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrDNd3H2XLcM"
      },
      "source": [
        "test = [e.split(\".\")[0] for e in test.split(\"\\n\")[:-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCEJQyW8F0bB"
      },
      "source": [
        "- Create train descriptions dictionary which will be similar to earlier one but having only train samples\n",
        "- Add startseq + endseq\n",
        "- Training have different dict. & testing have different dict. Now we load description from description.txt which was saved on hard disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcTK_wN0XUn0"
      },
      "source": [
        "train_descriptions = {}\n",
        "\n",
        "for t in train:\n",
        "  train_descriptions[t] = []\n",
        "\n",
        "  for cap in descriptions[t]:\n",
        "    cap_to_append = \"startseq \" + cap + \" endseq\"         \n",
        "    train_descriptions[t].append (cap_to_append)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIuPK54sYeBw"
      },
      "source": [
        "# Mapping descriptions to images\n",
        "\n",
        "train_descriptions['1000268201_693b08cb0e']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVfUVi5-Y3iE"
      },
      "source": [
        "images = flickr_path = \"Images/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuIrtBqCZIb1"
      },
      "source": [
        "**Model for preprocessing**\n",
        "- ResNet has all the information about images, it is a pretrained transfer learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMtvzpTiZP8w"
      },
      "source": [
        "model = ResNet50(weights=\"imagenet\", input_shape=(224,224,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E9kb7wkaLiI"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ML3PlfdG93q"
      },
      "source": [
        "- Create a new model by removing the last layer(output layer of 1000 classes) from resnet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj9mz9RQaP5s"
      },
      "source": [
        "model_new = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCf-SDwtanYs"
      },
      "source": [
        "model_new.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuuxYlooHItv"
      },
      "source": [
        "- This model was trained on the image dataset i.e. 1000 different classes of images. we removed the last layer to get rid of bottleneck features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjGwoPguHnrS"
      },
      "source": [
        "-- Now preprocessing images & converting my images to arrays then i did dimensioning and after that converting them to matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL2YZvzfaxcj"
      },
      "source": [
        "def preprocess_image(img):\n",
        "  img = image.load_img(img, target_size=(224,224))\n",
        "  img = image.img_to_array(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  img = preprocess_input(img)\n",
        "  return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzear5IP8fxJ"
      },
      "source": [
        "def encode_image(img):\n",
        "  img = preprocess_image(img)\n",
        "  feature_vector = model_new.predict(img)\n",
        "  feature_vector = feature_vector.reshape(feature_vector.shape[1])\n",
        "  return feature_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT-m1WuJ-Oq9"
      },
      "source": [
        "# Encoding each image in train set using Resnet\n",
        "\n",
        "encoding_train = {}\n",
        "\n",
        "for index, img in enumerate(train):\n",
        "\n",
        "  img = \"/content/Flickr_Data/Images/{}.jpg\".format(train[index])\n",
        "  encoding_train[img[len(images):]] = encode_image(img)\n",
        "\n",
        "  if index%100==0:\n",
        "        print (\"Encoding image-\" + str(index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsEwqdoSIZwq"
      },
      "source": [
        "- Encoding the images so they are basically given a number to it. And save the training features to the disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUNNKfJpGadq"
      },
      "source": [
        "# Save the train features to disk\n",
        "\n",
        "with open (\"./encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
        "   pickle.dump(encoding_train, encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMQ7E6ENIHoT"
      },
      "source": [
        "# Encoding test images\n",
        "\n",
        "encoding_test = {}\n",
        "\n",
        "for index, img in enumerate(test):\n",
        "\n",
        "  img = \"/content/Flickr_Data/Images/{}.jpg\".format(test[index])\n",
        "  encoding_test[img[len(images):]] = encode_image(img)\n",
        "\n",
        "  if index%100==0:\n",
        "        print (\"Encoding image-\" + str(index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzFUoAHYIs7d"
      },
      "source": [
        "# Save the bottleneck train features to disk\n",
        "\n",
        "with open (\"./encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
        "  pickle.dump(encoding_test, encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF4kFLFzJjn4"
      },
      "source": [
        "# Load the train images features from disk\n",
        "\n",
        "with open (\"./encoded_train_images.pkl\", \"rb\") as encoded_pickle:\n",
        "  encoding_train=pickle.load(encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCtkI-6XJ_Pl"
      },
      "source": [
        "encoding_train['2513260012_03d33305cf.jpg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HatlngydKKGa"
      },
      "source": [
        "# Load the test image features from disk\n",
        "\n",
        "with open (\"./encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
        "  encoding_test = pickle.load(encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9mgLpCsI2xv"
      },
      "source": [
        "- Now I have to data process my captions. Captions are something we want to predict so during training period caption will be target variable for our model but the prediction of entire caption given the images does not happen all at once, so we will be predicting the captions word by word.\n",
        "\n",
        "- One word is generated then RNN is going to generate oter word based on the sequence it will follow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXKlpRs_KaYX"
      },
      "source": [
        "# word_to_index is mapping between each unique word in all_vocab to int value & index_to_word is vice versa.\n",
        "\n",
        "index = 1\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "\n",
        "\n",
        "for e in all_vocab:\n",
        "  word_to_index[e] = index\n",
        "  index_to_word[index] = e\n",
        "  index+= 1 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Di-NfcGLZnv"
      },
      "source": [
        "word_to_index ['above']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rHtyyAaLegE"
      },
      "source": [
        "index_to_word [225]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKm3Wra1LiCK"
      },
      "source": [
        "len(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFIg919KLl49"
      },
      "source": [
        "# Adding startseq & endseq. Our machine doesn't understand words so convert these to indexes.\n",
        "\n",
        "word_to_index ['startseq'] = 1846\n",
        "word_to_index ['endseq'] = 1847\n",
        "\n",
        "index_to_word [1846] = 'startseq'\n",
        "index_to_word [1847] = 'endseq'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFjEsNorMJ8x"
      },
      "source": [
        "# vocab_size is total vocabulary len +1 because we will append 0's as well\n",
        "\n",
        "vocab_size = len(index_to_word)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rllIoLPhMi5v"
      },
      "source": [
        "#Now again making list of everything & storing max length to each one of them.\n",
        "\n",
        "all_captions_len = []\n",
        "\n",
        "for key in train_descriptions.keys():\n",
        "    for cap in train_descriptions[key]:\n",
        "        all_captions_len.append(len(cap.split()))\n",
        "\n",
        "max_len = max(all_captions_len)\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6csCkGNKTeH"
      },
      "source": [
        "- Now using Data Generator because caption has to be generated through images. I am giving image so it has to create a sequence of words which will be able to find what exact caption to be given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQR6TS82NGEB"
      },
      "source": [
        "def data_generator (train_descriptions, encoding_train, word_to_index, max_len, num_photos_per_batch):\n",
        "\n",
        "  x1, x2, y=[], [], []\n",
        "  n = 0\n",
        "\n",
        "  while True:\n",
        "\n",
        "    for key, desc_list in train_descriptions.items():\n",
        "      n += 1\n",
        "\n",
        "      photo = encoding_train[key+\".jpg\"]\n",
        "\n",
        "      for desc in desc_list:\n",
        "\n",
        "        seq = [word_to_index[word] for word in desc.split() if word in word_to_index]\n",
        "\n",
        "        for i in range(1,len(seq)):\n",
        "\n",
        "          in_seq = seq[0:i]\n",
        "          out_seq = seq[i]\n",
        "\n",
        "          in_seq = pad_sequences ([in_seq], maxlen = max_len, value = 0, padding = 'post')[0]          # adding 0 padding because I need to start from 0.\n",
        "\n",
        "          out_seq = to_categorical ([out_seq], num_classes = vocab_size)[0]\n",
        "\n",
        "          x1.append(photo)\n",
        "          x2.append(in_seq)\n",
        "          y.append(out_seq)\n",
        "\n",
        "      if n == num_photos_per_batch:                                      # It will start batch processing part of it now.\n",
        "        yield [[np.array(x1), np.array(x2)], np.array(y)]                # Also it will take every caption as a batch.\n",
        "        x1, x2, y = [], [], []\n",
        "        n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRhIUu_qfAvU"
      },
      "source": [
        "f = open(\"glove.6B.50d.txt\", encoding = 'utf8')\n",
        "f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Ij1EwxLppv"
      },
      "source": [
        "- Creating dictionary for embedding means creating vector for words that are created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9Bf-FrYfa3i"
      },
      "source": [
        "embedding_index = {}\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word =  values[0]\n",
        "  Coefs = np.array(values[1: ], dtype=\"float\")\n",
        "\n",
        "  embedding_index[word] = Coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fROkFOTaheM5"
      },
      "source": [
        "# Converting words into vector directly - (Embedding Layer Output)\n",
        "\n",
        "def get_embedding_output():\n",
        "  emb_dim = 50\n",
        "  embedding_output = np.zeros((vocab_size, emb_dim))\n",
        "\n",
        "  for word, idx in word_to_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "  \n",
        "    if embedding_vector is not None:\n",
        "      embedding_output[idx] = embedding_vector\n",
        "      \n",
        "  return embedding_output\n",
        "\n",
        "embedding_output = get_embedding_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Caf0eAs_jEEt"
      },
      "source": [
        "embedding_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfdVlqbajxLB"
      },
      "source": [
        "embedding_output.shape                    # checking shape of output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYrBdXAjkBkJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOJhRc70j03s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldAWQ9F5kMO3"
      },
      "source": [
        "**Defining Model**\n",
        "\n",
        "- Since the input consists of 2 parts, an image vector & partial caption, we cannot use sequential model so we create custom.\n",
        "\n",
        "- Features are extracted through CNN. Now we are going to decode everything. So every image we see, the objects are identified, the features are extracted from CNN and RNN will be decoding each and every image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WknhJEuIkjPA"
      },
      "source": [
        "# Image feature extraction model, inputs image feature vector\n",
        "\n",
        "input_img_feat = Input(shape=(2048))\n",
        "inp_img1 = Dropout(0.3)(input_img_feat)\n",
        "inp_img2 = Dense(256, activation='relu')(inp_img1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlbywapAlWef"
      },
      "source": [
        "# Partial caption sequence model, inputs captions\n",
        "\n",
        "input_cap = Input(shape = max_len)\n",
        "inp_cap1 = Embedding(input_dim= vocab_size, output_dim= 50, mask_zero= True)(input_cap)\n",
        "inp_cap2 = Dropout(0.3)(inp_cap1)\n",
        "inp_cap3 = LSTM(256)(inp_cap2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oys3ZHHRmFTL"
      },
      "source": [
        "decoder1 = add([inp_img2,inp_cap3])\n",
        "decoder2 = Dense(256, activation= 'relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation= 'softmax')(decoder2)\n",
        "\n",
        "# Merge two networks\n",
        "model = Model(inputs = [input_img_feat, input_cap], outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jCemYFFnDJu"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgZ0hbbXnH0f"
      },
      "source": [
        "model.layers[2].set_weights([embedding_output])\n",
        "model.layers[2].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeQE3K6pnpBg"
      },
      "source": [
        "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJlsBFU6n7Uy"
      },
      "source": [
        "epochs = 20                                   # Hyperparameter is the epoch.\n",
        "number_pics_per_batch = 3\n",
        "steps = len(train_descriptions)               # what steps to be taken. Step size is also learning rate."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I1Rt9ByoIyt"
      },
      "source": [
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, encoding_train, word_to_index, max_len, number_pics_per_batch)\n",
        "\n",
        "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "    model.save('./model_' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRzbFsHoQ_a7"
      },
      "source": [
        "# Taking model from after 19th epoch\n",
        "\n",
        "model = model(\"./model_19.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDV27udXROBZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh4CDX_nNGy1"
      },
      "source": [
        "- Creating a function for predicting captions. Image I am giving will be converted to word, then word to index & finally it will be converted to the caption. It will be splited and I have maintained all the ratio of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4QNVypIRWRZ"
      },
      "source": [
        "def predict_caption(photo):\n",
        "  in_text = \"startseq\"\n",
        "\n",
        "  for i in range(max_len):\n",
        "    sequence = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
        "    sequence = pad_sequences([sequence], maxlen = max_len, padding = 'post')\n",
        "\n",
        "    ypred = model.predict([photo, sequence])\n",
        "    ypred = ypred.argmax()\n",
        "    word = index_to_word[ypred]\n",
        "    in_text = ' ' + word\n",
        "\n",
        "    if word = 'endseq':\n",
        "      break\n",
        "\n",
        "   final_caption = in_text.split()\n",
        "   final_caption = final_caption[1 :-1]\n",
        "   final_caption = ' '.join(final_caption)\n",
        "\n",
        "   return final_caption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6gy3piwS71Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sisWhqcjS9_a"
      },
      "source": [
        "for i in range(20):                           # Doing it for the first 20 images.\n",
        "  rn = np.random.randint(0, 1000)\n",
        "  img_name = list(encoding_test.keys())[rn]\n",
        "  photo = encoding_test[img_name].reshape((1, 2048))\n",
        "\n",
        "  i = plt.imread(images + img_name)\n",
        "  plt.imshow(i)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "  caption = predict_caption(photo)\n",
        "  print(caption)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}